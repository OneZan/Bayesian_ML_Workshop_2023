{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parametric Priors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Tomaremos como ejemplo de aplicación de los metódos de shrinkage el modelo de ajuste lineal $Y = \\beta^{T} X$. El modelo lineal es en general una primera opción para unaa regresión dada sus simpleza y que es facil de interpretar. Sin embargo uno suele incorporar un gran número de variables que quizás sobre parametrizan el problema, y en ese caso las inferencias pueden resultar poco satisfactorias dado que suceden dos cosas:\n",
    "1) Al introducir más parámetros, tenemos mayor incerteza al hacer las posteriors predictivas y por ende mayor varianza en las predicciones. \n",
    "2) El hecho de que tengamos muchos parámeros hace que no podamos determinar cuáles son los parámetros más relevantes y mejor explican la relación que hay en la naturaleza del problema. Por lo tanto podemos sacrificar algunos de ellos para determinar cuáles son las mejores variables \"explicativas\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One first solution (Backward stepwise selection)\n",
    "\n",
    "Una vez que fiteamos todo el modelo podemos evaluar si el MLE de parámetros obtenidos es raro en relación a lo que nos presentan los datos, y en ese caso descartarlos. En detalle tenemos que un modelo lineal tiene como parámetros $\\beta$ óptimos a:\n",
    "$$\n",
    "\\beta_{MlE} = (X^{T} X)^{-1} X^{T} y\\\\\n",
    "Var(\\beta) = (X^{T}X)^{-1}\\sigma^{2}\n",
    "$$\n",
    "donde $\\sigma^{2}$ es la incerteza de los datos, que en caso de no conocerla podemos utilizar su MLE, $\\sigma_{MLE} = \\frac{1}{N-p-1} \\sum_{i=1}^{N}(y_{i} - \\bar{y})^{2}.$\n",
    "\n",
    "Calculemos estos valores para una base de datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Imports\n",
    "\n",
    "from numpy import *\n",
    "from scipy.stats import binom,beta\n",
    "from scipy.optimize import curve_fit\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.special import loggamma\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../bin/data_sets/cancer.csv\")\n",
    "df_train = df[df[\"train\"] == \"T\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lcavol</th>\n",
       "      <th>lweight</th>\n",
       "      <th>age</th>\n",
       "      <th>lbph</th>\n",
       "      <th>svi</th>\n",
       "      <th>lcp</th>\n",
       "      <th>gleason</th>\n",
       "      <th>pgg45</th>\n",
       "      <th>lpsa</th>\n",
       "      <th>train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.579818</td>\n",
       "      <td>2.769459</td>\n",
       "      <td>50</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.430783</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.994252</td>\n",
       "      <td>3.319626</td>\n",
       "      <td>58</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.162519</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.510826</td>\n",
       "      <td>2.691243</td>\n",
       "      <td>74</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>-0.162519</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.203973</td>\n",
       "      <td>3.282789</td>\n",
       "      <td>58</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.162519</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.751416</td>\n",
       "      <td>3.432373</td>\n",
       "      <td>62</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.371564</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>3.246491</td>\n",
       "      <td>4.101817</td>\n",
       "      <td>68</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>4.029806</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>2.532903</td>\n",
       "      <td>3.677566</td>\n",
       "      <td>61</td>\n",
       "      <td>1.348073</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>4.129551</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>2.830268</td>\n",
       "      <td>3.876396</td>\n",
       "      <td>68</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>1</td>\n",
       "      <td>1.321756</td>\n",
       "      <td>7</td>\n",
       "      <td>60</td>\n",
       "      <td>4.385147</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>3.821004</td>\n",
       "      <td>3.896909</td>\n",
       "      <td>44</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>1</td>\n",
       "      <td>2.169054</td>\n",
       "      <td>7</td>\n",
       "      <td>40</td>\n",
       "      <td>4.684443</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>2.882564</td>\n",
       "      <td>3.773910</td>\n",
       "      <td>68</td>\n",
       "      <td>1.558145</td>\n",
       "      <td>1</td>\n",
       "      <td>1.558145</td>\n",
       "      <td>7</td>\n",
       "      <td>80</td>\n",
       "      <td>5.477509</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      lcavol   lweight  age      lbph  svi       lcp  gleason  pgg45  \\\n",
       "0  -0.579818  2.769459   50 -1.386294    0 -1.386294        6      0   \n",
       "1  -0.994252  3.319626   58 -1.386294    0 -1.386294        6      0   \n",
       "2  -0.510826  2.691243   74 -1.386294    0 -1.386294        7     20   \n",
       "3  -1.203973  3.282789   58 -1.386294    0 -1.386294        6      0   \n",
       "4   0.751416  3.432373   62 -1.386294    0 -1.386294        6      0   \n",
       "..       ...       ...  ...       ...  ...       ...      ...    ...   \n",
       "90  3.246491  4.101817   68 -1.386294    0 -1.386294        6      0   \n",
       "91  2.532903  3.677566   61  1.348073    1 -1.386294        7     15   \n",
       "92  2.830268  3.876396   68 -1.386294    1  1.321756        7     60   \n",
       "93  3.821004  3.896909   44 -1.386294    1  2.169054        7     40   \n",
       "95  2.882564  3.773910   68  1.558145    1  1.558145        7     80   \n",
       "\n",
       "        lpsa train  \n",
       "0  -0.430783     T  \n",
       "1  -0.162519     T  \n",
       "2  -0.162519     T  \n",
       "3  -0.162519     T  \n",
       "4   0.371564     T  \n",
       "..       ...   ...  \n",
       "90  4.029806     T  \n",
       "91  4.129551     T  \n",
       "92  4.385147     T  \n",
       "93  4.684443     T  \n",
       "95  5.477509     T  \n",
       "\n",
       "[67 rows x 10 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n",
    "\n",
    "Estos datos buscan ver qué variables influyen en el cancer de prostata, las variables son: \n",
    "* log cancer volum : \"lcavol\"\n",
    "* log prostate weight: \"lweight\"\n",
    "* age\n",
    "*  log of the amount of benign postatic hyperplasia: \"lbph\"\n",
    "* seminal vesicle invasion: \"svi\"\n",
    "* log of capsular penetration: \"lcp\"\n",
    "* Gleadon score: \"Gleason\".\n",
    "* Percent of Gleason scores 4 or 5: \"ppgg5\"\n",
    "\n",
    "La variable apredecir es log of prostate-specific antigen: \"lpsa\"\n",
    "\n",
    "Vemos de los datos que svi es binaria y que Gleason es una variable categórica ordenada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train[['lcavol', 'lweight', 'age', 'lbph', 'svi', 'lcp', 'gleason', 'pgg45']]\n",
    "X = (X-X.mean())/X.std()\n",
    "X[\"intercept\"] = 1\n",
    "\n",
    "Y = df_train[\"lpsa\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_X = X.T.dot(X)\n",
    "X_inv = pd.DataFrame(linalg.pinv(aux_X.values), aux_X.columns, aux_X.index)\n",
    "beta_MLE = (X_inv.dot(X.T)).dot(Y)\n",
    "var_data = 1/(len(Y)-len(X.T))*((Y-Y.mean())**2).sum()\n",
    "std_betas = sqrt(X_inv*var_data)\n",
    "std_diag = pd.DataFrame(diag(std_betas),std_betas.columns)\n",
    "\n",
    "diag_X_inv = pd.DataFrame(diag(X_inv), X_inv.columns)\n",
    "\n",
    "\n",
    "Z_score =pd.DataFrame(beta_MLE.values * 1/sqrt(var_data*diag_X_inv).values, index=diag_X_inv.index)\n",
    "Z_score = pd.DataFrame(diag(Z_score), index = beta_MLE.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los parametros valen: \n",
      " lcavol       0.716407\n",
      "lweight      0.292642\n",
      "age         -0.142550\n",
      "lbph         0.212008\n",
      "svi          0.309620\n",
      "lcp         -0.289006\n",
      "gleason     -0.020914\n",
      "pgg45        0.277346\n",
      "intercept    2.452345\n",
      "dtype: float64\n",
      "Su std es: \n",
      "                   0\n",
      "lcavol     0.241484\n",
      "lweight    0.192434\n",
      "age        0.184719\n",
      "lbph       0.186536\n",
      "svi        0.226812\n",
      "lcp        0.280017\n",
      "gleason    0.257902\n",
      "pgg45      0.288679\n",
      "intercept  0.157405\n"
     ]
    }
   ],
   "source": [
    "print(\"Los parametros valen: \\n\",beta_MLE)\n",
    "print(\"Su std es: \\n\",std_diag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí ya calculamos las posteriors de los parametros, ahora lo que podemos hacer es ver cuáles de ellos podrían ser \"0\" con probabilidad razonable e ir tirandolos para quedarnos con un modelo que \"necesite todos los parámetros. Para eso construimos el \"Z-Score\" el cual es una renormalización de cada parametro de la forma:\n",
    "$$\n",
    "z_{j} = \\frac{\\beta_{j}}{\\sigma \\sqrt{{X^{T}X}_{jj}}}\n",
    "$$\n",
    "\n",
    "Los $z_{j}$ son un escaleo de las distribuciones posterior de cada $\\beta_{j}$ con el valor que toman podemos ver qué tan cerca de cero están y ver qué tan probable es que realmente sean cero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lcavol</th>\n",
       "      <td>2.966684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lweight</th>\n",
       "      <td>1.520738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>-0.771710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lbph</th>\n",
       "      <td>1.136548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svi</th>\n",
       "      <td>1.365096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lcp</th>\n",
       "      <td>-1.032099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gleason</th>\n",
       "      <td>-0.081091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pgg45</th>\n",
       "      <td>0.960742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept</th>\n",
       "      <td>15.579793</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0\n",
       "lcavol      2.966684\n",
       "lweight     1.520738\n",
       "age        -0.771710\n",
       "lbph        1.136548\n",
       "svi         1.365096\n",
       "lcp        -1.032099\n",
       "gleason    -0.081091\n",
       "pgg45       0.960742\n",
       "intercept  15.579793"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los valores más chicos en módulo son los que serán menos relevantes, en este caso gelason age y pgg 45. De no estar satisfechos con el modelo los podemos remover y ver cómo se ven afectadas los parámetros de las otras variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementemos lo mismo con Sklearn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X = df_train[['lcavol', 'lweight', 'age', 'lbph', 'svi', 'lcp', 'gleason', 'pgg45']]\n",
    "scaler = StandardScaler().fit(X)\n",
    "X = scaler.transform(X)\n",
    "reg = LinearRegression().fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.71104059,  0.29045029, -0.14148182,  0.21041951,  0.30730025,\n",
       "        -0.28684075, -0.02075686,  0.27526843]),\n",
       " 2.4523450850746262)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.coef_,reg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shrinage\n",
    "\n",
    "#### Ridge regression\n",
    "\n",
    "Descartar variables coincide con la idea de que alguno de sus parámetros sean cero y quedarnos con aquellas variables cuyos parámetros sean más significativos. Otra idea es hacer que los parámetros no puedan tomar valores muy grandes penalizando en la función de costo por su valor absoluto. Es decir que la loss function ahora será:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\left\\{ \\sum_{i=1}^{N}(y_{i} - \\beta_{0} + \\sum_{j=1}^{p} x_{ij}\\beta_{j})^{2} + \\lambda \\sum_{j=1}^{p}\\beta_{j}^{2}\\right\\}\n",
    "$$\n",
    "\n",
    "Esto es equivalente a proponer un hyperparámetro $\\lambda$ que gobierna la prior de los $\\beta$, es decir que la posterior de los $\\beta$ es:\n",
    "\n",
    "$$\n",
    "P(\\beta|D,\\sigma^{2},\\lambda) \\propto P(D|\\beta,\\sigma^{2}\\lambda) P(\\beta|\\sigma^{2}\\lambda)  = \\exp\\left\\{-\\frac{1}{2\\sigma^{2}}\\sum_{i=1}^{N}(y_{i} - \\beta_{0} + \\sum_{j=1}^{p} x_{ij}\\beta_{j})^{2}\\right\\} \\exp\\left\\{-\\frac{\\lambda}{2\\sigma^{2}} \\sum_{j=1}^{p} \\beta_{j}^{2}\\right\\}\n",
    "$$\n",
    "\n",
    "Hagamos el mismo fiteo pero con este hyperparámetro seteado a $1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "reg_Ridge = Ridge(alpha=1.0)\n",
    "reg_Ridge = reg_Ridge.fit(X, Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.68540969,  0.28959545, -0.13430643,  0.20841057,  0.30162494,\n",
       "        -0.25453234, -0.0112517 ,  0.25598543]),\n",
       " 2.4523450850746262)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_Ridge.coef_,reg_Ridge.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los valores de los parámetros son todos más chicos, pero no en la misma proporción. La idea es que los que menos importen tomaran valores más chicos dejanod lugar a los más relevantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso\n",
    "\n",
    "Lasso propone lo miso que antes pero la unción de costo es $|\\beta_{j}|$ en lugar del cuadrado\n",
    "\n",
    "$$\n",
    "P(\\beta|D,\\sigma^{2},\\lambda) \\propto P(D|\\beta,\\sigma^{2}\\lambda) P(\\beta|\\sigma^{2}\\lambda)  = \\exp\\left\\{-\\frac{1}{2\\sigma^{2}}\\sum_{i=1}^{N}(y_{i} - \\beta_{0} + \\sum_{j=1}^{p} x_{ij}\\beta_{j})^{2}\\right\\} \\exp\\left\\{-\\frac{\\lambda}{2\\sigma^{2}} \\sum_{j=1}^{p} |\\beta_{j}|\\right\\}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "reg_Lasso = Lasso(alpha=0.1)\n",
    "reg_Lasso = reg_Lasso.fit(X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.57065706,  0.22862578, -0.        ,  0.10501276,  0.17097605,\n",
       "         0.        ,  0.        ,  0.06532039]),\n",
       " 2.4523450850746262)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_Lasso.coef_,reg_Lasso.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como es de esperar el efecto es más fuerte en el parámetro $\\alpha$ o $\\lambda$ dado que no tenemos potencia de $\\beta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observaciones\n",
    "\n",
    "Esta frma de introducir un parámetro que llamaremos _hyperparámetro_ el cual regula el comportamiento de las variables que queremos estimar es una perilla que nos puede ayudar a fittear mejor nuestro modelo. Es el hecho de poner una variable que vincula a los parámetros entre sí por fuera de los datos que hace que la estructura del modelo sea más compleja y permita modelar los datos de una manera más interesante. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Priors parámetricas con interpretación. Modelos jerárquicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
